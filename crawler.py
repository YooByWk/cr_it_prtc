# íŒŒì¼ëª…: crawler.py
import os
import feedparser
import yaml
import sqlite3
from datetime import datetime

CONFIG_PATH = "config.yaml"
OUT_DIR = "out_md"
DB_PATH = "news.db"

def init_db():
    """SQLite DB ì´ˆê¸°í™”"""
    conn = sqlite3.connect(DB_PATH)
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS news(
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            title TEXT,
            link TEXT UNIQUE,
            summary TEXT,
            source TEXT,
            pub_date TEXT
        )
    """)
    conn.commit()
    return conn

def save_to_db(conn, title, link, summary, source, pub_date):
    """DB ì €ì¥ (ì¤‘ë³µ ë°©ì§€: link ê¸°ì¤€)"""
    try:
        cur = conn.cursor()
        cur.execute("""
            INSERT OR IGNORE INTO news (title, link, summary, source, pub_date)
            VALUES (?, ?, ?, ?, ?)
        """, (title, link, summary, source, pub_date))
        conn.commit()
    except Exception as e:
        print(f"DB ì €ì¥ ì˜¤ë¥˜: {e}")

def update_home_md(date_str, time_str):
    """Home.md ê°±ì‹ : ìµœì‹  ë‰´ìŠ¤ ë§í¬ ë§¨ ìœ„ ëˆ„ì """
    home_file = os.path.join(OUT_DIR, "Home.md")
    if os.path.exists(home_file):
        with open(home_file, "r", encoding="utf-8") as f:
            home_lines = f.readlines()
    else:
        home_lines = [
            "# ğŸ“° IT ë‰´ìŠ¤ ìœ„í‚¤ í™ˆ\n\n",
            "ìë™ ìˆ˜ì§‘ëœ IT ë‰´ìŠ¤ ì•„ì¹´ì´ë¸Œì…ë‹ˆë‹¤.\n\n",
            "## ğŸ“… ìµœê·¼ ë‰´ìŠ¤\n"
        ]

    # Daily ë§í¬ ì¶”ê°€
    today_news_link = f"- [{date_str} {time_str}ì‹œ IT ë‰´ìŠ¤ ìš”ì•½](Daily/{date_str}-{time_str}.md)\n"
    home_lines.insert(3, today_news_link)  # ìµœê·¼ ë‰´ìŠ¤ ì„¹ì…˜ ë°”ë¡œ ì•„ë˜

    # Home.md ë‹¤ì‹œ ì“°ê¸°
    with open(home_file, "w", encoding="utf-8") as f:
        f.writelines(home_lines)

def main():
    # ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
    with open(CONFIG_PATH, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    feeds = cfg.get("feeds", [])
    os.makedirs(OUT_DIR, exist_ok=True)
    os.makedirs(os.path.join(OUT_DIR, "Daily"), exist_ok=True)

    # ë‚ ì§œ/ì‹œê°„ ê¸°ë°˜ íŒŒì¼
    now = datetime.now()
    date_str = now.strftime("%Y-%m-%d")
    time_str = now.strftime("%H")  # 09 or 18
    out_file = os.path.join(OUT_DIR, "Daily", f"{date_str}-{time_str}.md")

    # DB ì—°ê²°
    conn = init_db()

    with open(out_file, "w", encoding="utf-8") as f:
        f.write(f"# {date_str} {time_str}ì‹œ IT ë‰´ìŠ¤ ìš”ì•½\n\n")

        for feed_url in feeds:
            d = feedparser.parse(feed_url)
            source = d.feed.get("title", feed_url)

            f.write(f"## {source}\n\n")

            for entry in d.entries[:10]:
                title = entry.title
                link = entry.link
                summary = getattr(entry, "summary", "").strip()
                pub_date = getattr(entry, "published", now.strftime("%Y-%m-%d %H:%M"))

                if summary:
                    summary = " ".join(summary.split())[:200]  # 200ì ì œí•œ
                else:
                    summary = "(ìš”ì•½ ì—†ìŒ)"

                # DB ì €ì¥
                save_to_db(conn, title, link, summary, source, pub_date)

                # Markdown ì‘ì„±
                f.write(f"- **{title}**\n")
                f.write(f"  - {summary}\n")
                f.write(f"  - [ì›ë¬¸ ë§í¬]({link})\n\n")

    # Home.md ê°±ì‹ 
    update_home_md(date_str, time_str)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {out_file}")

if __name__ == "__main__":
    main()
